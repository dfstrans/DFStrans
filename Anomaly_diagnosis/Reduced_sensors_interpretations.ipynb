{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb8fa34c",
   "metadata": {},
   "source": [
    "# FIRST, LET'S TRAIN DFSTRANS WITH LESS SENSORS (JUST FOR CLARITY OF INTERPRETATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa05bc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Epoch 1 loss: 0.13435 \n",
      "Precision train 0.53125 , Recall train 0.05923 , F1 train 0.10658\n",
      "test loss: 0.09143 \n",
      "Precision test 1.00000 , Recall test 0.21138 , F1 test 0.34899\n",
      "Epoch 2 loss: 0.06393 \n",
      "Precision train 0.89412 , Recall train 0.52962 , F1 train 0.66521\n",
      "test loss: 0.05733 \n",
      "Precision test 0.87912 , Recall test 0.65041 , F1 test 0.74766\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-038c323479c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;31m# zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    189\u001b[0m                             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m                         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "import os\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import *\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.backends import cudnn\n",
    "from torch.autograd import Variable\n",
    "import torch.multiprocessing as mp\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "gpu = str(2)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu\n",
    "print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "\n",
    "\n",
    "\n",
    "# custom class to generate batch data on the fly\n",
    "class CustomDataGenerator(Dataset):\n",
    "    def __init__(self, path, data_index, sequence_length, time_steps, window_length,\n",
    "                 window_step, n_channels):\n",
    "        \"\"\" Method called at the initialization of the class (Constructor).\n",
    "\n",
    "            Args:\n",
    "                path (string): path where the dataset is located\n",
    "                data_index (array): index list of the dataset\n",
    "                batch_size (int): the size of the batch\n",
    "                sequence_length (int): the length of the time series\n",
    "                time_steps (int): number of windows in which the time series are divided\n",
    "                window_length (int): the length (data point) of the window\n",
    "                window_step (int): indicates how much to slice de window over the time series\n",
    "                n_channels (int): number of sensors\n",
    "                isTrainig (bool): whether the generator is for training or test\n",
    "\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.data_index = data_index\n",
    "        self.sequence_length = sequence_length\n",
    "        self.time_steps = time_steps\n",
    "        self.window_length = window_length\n",
    "        self.window_step = window_step\n",
    "        self.n_channels = n_channels\n",
    "        self.len = data_index.shape[0]\n",
    "        \n",
    "        # divide data for each sensor channel\n",
    "        output_params = {'Alpha': 0, 'Ax': 1, 'Ay': 2, 'Az': 3, 'Fc': 4, 'Fcw': 5, 'FrictionCW': 6, 'FrictionCabin': 7,\n",
    "                 'Fsupport': 8, 'Id': 9, 'Iq': 10, 'Omega': 11, 'Phi': 12, 'Vc': 13, 'Vcw': 14, 'Vd': 15, 'Vq': 16,\n",
    "                 'Zc': 17, 'Zcw': 18, 'pulleyAz': 19}\n",
    "        \n",
    "        params_to_show = ['Ax', 'Ay', 'Az', 'Id', 'Iq','Omega', 'Phi', 'Vc', 'Vcw', 'Vd', 'Vq', 'Zc', 'Zcw']\n",
    "        \n",
    "        self.params_id = [output_params[param] for param in params_to_show]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Method called at the time of requiring the number of batches per epoch\n",
    "\n",
    "                Returns:\n",
    "                    int: number of batches per epoch\n",
    "\n",
    "            \"\"\"\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        index = self.data_index[idx]\n",
    "\n",
    "        col_size = self.sequence_length * self.n_channels\n",
    "        with h5py.File(self.path, 'r') as hf:\n",
    "            x = hf['dataset'][index, :col_size]\n",
    "            y = hf['dataset'][index, -1:]\n",
    "\n",
    "        # reshape data to 4D (time_steps, window_length, channels)\n",
    "        x = reshape_data(x, self.sequence_length, self.time_steps, self.window_length, self.window_step,\n",
    "                          self.n_channels)\n",
    "        \n",
    "        input_data_x_list = []\n",
    "        for channel in self.params_id:\n",
    "            input_data_x_list.append(x[:, :, channel:channel + 1])\n",
    "        \n",
    "        return input_data_x_list, y\n",
    "\n",
    "\n",
    "def reshape_data(data, sequence_length, time_steps, window_length, window_step, n_channels):\n",
    "\n",
    "    x_reshaped = np.zeros((time_steps, window_length, n_channels))\n",
    "\n",
    "    current_simulation_values = np.zeros((time_steps, window_length, n_channels))\n",
    "    for channel in range(n_channels):\n",
    "        channel_start_point = channel * sequence_length\n",
    "        channel_end_point = channel_start_point + sequence_length\n",
    "        param_values = data[channel_start_point:channel_end_point]\n",
    "\n",
    "        for step in range(time_steps):\n",
    "            time_step_start_point = step * window_step\n",
    "            time_step_stop_point = time_step_start_point + window_length\n",
    "            time_step_values = param_values[time_step_start_point:time_step_stop_point]\n",
    "            current_simulation_values[step, :, channel] = time_step_values\n",
    "\n",
    "        x_reshaped[:, :, :] = current_simulation_values\n",
    "\n",
    "    return x_reshaped\n",
    "      \n",
    "def scale_data_between_a_b(batch_x,a,b):\n",
    "    '''Scales data between values (a,b)\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "        batch_x (1D mumpy array): features data of the current batch\n",
    "        a (int): min value of transformed data\n",
    "        b (int): max value of transformed data\n",
    "    \n",
    "    Returns:\n",
    "        \n",
    "        1D numpy array: Transformed data\n",
    "    '''\n",
    "    return (b-a)*((batch_x-torch.amin(batch_x))/(torch.amax(batch_x)-torch.amin(batch_x)))+a\n",
    "\n",
    "\n",
    "\n",
    "class DFTEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(DFTEncoding, self).__init__()\n",
    "        torch.pi = torch.acos(torch.zeros(1)).item() * 2\n",
    "        w_s = 2 * torch.pi / d_model\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        div_term = torch.arange(0, d_model, 2).float() // 2\n",
    "        pe[:, 1::2] = torch.sin(position * w_s * div_term)\n",
    "        pe[:, 0::2] = torch.cos(position * w_s * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.d_model = d_model\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pe_multidim = self.pe[1:x.size()[0] + 1, :].unsqueeze(-1)\n",
    "        pe_multidim[0] = math.sqrt(1 / self.d_model) * pe_multidim[0]\n",
    "        pe_multidim[-1] = math.sqrt(1 / self.d_model) * pe_multidim[-1]\n",
    "        pe_multidim[1:-1] = torch.mul(pe_multidim[1:-1], math.sqrt(2 / self.d_model))\n",
    "\n",
    "        pe_multidim.repeat(1, x.size()[1], 1, x.size()[3]).size()\n",
    "\n",
    "        return x + pe_multidim\n",
    "\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "\n",
    "    raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))\n",
    "def _get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        return x.view(batch_size, -1)\n",
    "\n",
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module, batch_first=True):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "\n",
    "        x_reshape = x.contiguous().view(-1, *(x.size()[2:]))\n",
    "        y = self.module(x_reshape)\n",
    "\n",
    "        # We have to reshape Y\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(-1))  # (samples, timesteps, output_size)\n",
    "        else:\n",
    "            y = y.contiguous().view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
    "\n",
    "        return y\n",
    "    \n",
    "class MultiHead1DCNN(nn.Module):\n",
    "\n",
    "    def __init__(self,conv_filters = 20, time_steps = 80):\n",
    "        super(MultiHead1DCNN, self).__init__()\n",
    "\n",
    "        self.conv_filters = conv_filters\n",
    "        self.time_steps = time_steps\n",
    "\n",
    "        self.conv1d1 = nn.Conv1d(in_channels=1, out_channels=self.conv_filters, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(self.conv_filters,track_running_stats=False)\n",
    "        self.conv1d2 = nn.Conv1d(in_channels=self.conv_filters, out_channels=self.conv_filters, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(self.conv_filters,track_running_stats=False)\n",
    "        self.conv1d3 = nn.Conv1d(in_channels=self.conv_filters, out_channels=self.conv_filters, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(self.conv_filters,track_running_stats=False)\n",
    "        self.maxpool = nn.MaxPool1d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1d1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.bn1(x)\n",
    "        X = x.view(-1, x.size()[0] // self.time_steps, self.time_steps, self.conv_filters)\n",
    "        X = x.view(-1, *(x.size()[2:]))\n",
    "        x = self.conv1d2(x)\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.bn2(x)\n",
    "        X = x.view(-1, x.size()[0] // self.time_steps, self.time_steps, self.conv_filters)\n",
    "        X = x.view(-1, *(x.size()[2:]))\n",
    "        x = self.conv1d3(x)\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.bn3(x)\n",
    "        x = x.view(x.size()[0], x.size()[1], -1)\n",
    "        return x\n",
    "        \n",
    "class TemporalEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(TemporalEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(TemporalEncoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        \n",
    "\n",
    "        src2, weights = self.self_attn(src, src, src, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        \n",
    "        return src, weights\n",
    "    \n",
    "class TransTS(nn.Module):\n",
    "    def __init__(self,feature_size=240,num_layers=1,dropout=0.1,temp_gap = False):\n",
    "        super(TransTS, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.src_mask = None\n",
    "        self.encoder_layer = TemporalEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.temp_gap = temp_gap\n",
    "        \n",
    "\n",
    "    def forward(self,src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        output,weights = self.transformer_encoder(src,self.src_mask)#, self.src_mask)\n",
    "\n",
    "        output = output.permute(1,0,2) \n",
    "\n",
    "        return output,weights\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float(0.0)).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "\n",
    "    \n",
    "class SpatialEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(SpatialEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(SpatialEncoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        \n",
    "        src2, weights = self.self_attn(src, src, src, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        \n",
    "        return src, weights\n",
    "    \n",
    "class TransSensor(nn.Module):\n",
    "    def __init__(self, feature_size=240, num_layers=1, dropout=0.1):\n",
    "        super(TransSensor, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.encoder_layer = SpatialEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, src):\n",
    "\n",
    "        bs = src.size()[1]\n",
    "\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        output, weights = self.transformer_encoder(src, self.src_mask)\n",
    "\n",
    "        output = output.permute(1, 0, 2)\n",
    "\n",
    "        return output, weights\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float(0.0)).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "class DFSTrans_model(nn.Module):\n",
    "\n",
    "    def __init__(self, activation=\"relu\", d_model=240, dim_feedforward=2048,\n",
    "                 dropout=0.1,n_channels = 13,n_time_steps = 80,output_dim = 3120, n_units_l1 = 512, conv_filters = 20):\n",
    "        super(DFSTrans_model, self).__init__()\n",
    "        self.conv_cell = nn.ModuleList([MultiHead1DCNN(conv_filters = conv_filters, time_steps = n_time_steps) for i in range(n_channels)])\n",
    "        self.TimeDistributed_flatten = nn.ModuleList([TimeDistributed(Flatten) for i in range(n_channels)])\n",
    "        self.trace = []\n",
    "        self.TransformerTS_list = nn.ModuleList([TransTS() for i in range(n_channels)])\n",
    "        self.TransformerS_list = nn.ModuleList([TransSensor() for i in range(n_time_steps)])\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.pos_encoder = DFTEncoding(self.d_model)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.dense1 = nn.Linear(output_dim, n_units_l1)\n",
    "        self.dropout_out1 = nn.Dropout(dropout)\n",
    "        self.dense2 = nn.Linear(n_units_l1, 1)\n",
    "\n",
    "        self.n_time_steps = n_time_steps\n",
    "        self.n_channels = n_channels\n",
    "\n",
    "    def forward(self, input_x):\n",
    "\n",
    "        trace = []\n",
    "        for sensor_n in range(self.n_channels):\n",
    "            input_layer = input_x[sensor_n]\n",
    "            input_layer_reshape = input_layer.view(-1, *(input_layer.size()[2:]))\n",
    "            x = self.conv_cell[sensor_n](input_layer_reshape)\n",
    "            x = x.view(x.size()[0] // self.n_time_steps, self.n_time_steps, -1)\n",
    "            trace.append(x)\n",
    "\n",
    "        x = torch.stack(trace)\n",
    "        x = x.permute(2, 1, 3, 0)\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        input_ts = torch.clone(x)\n",
    "        input_sensor = torch.clone(x)\n",
    "        input_ts = input_ts.permute(3, 0, 1, 2)\n",
    "        input_sensor = input_sensor.permute(0, 3, 1, 2)\n",
    "\n",
    "        output_ts_list = []\n",
    "        ts_weights_list = []\n",
    "\n",
    "        for i in range(self.n_channels):\n",
    "            channel = input_ts[i, :, :, :]\n",
    "            trans_ts = self.TransformerTS_list[i]\n",
    "\n",
    "            output_ts, weights_ts = trans_ts(channel)\n",
    "            output_ts_list.append(output_ts)\n",
    "            ts_weights_list.append(weights_ts)\n",
    "\n",
    "        output_s_list = []\n",
    "        s_weights_list = []\n",
    "\n",
    "        for i in range(self.n_time_steps):\n",
    "            ts = input_sensor[i, :, :, :]\n",
    "            trans_s = self.TransformerS_list[i]\n",
    "\n",
    "            output_s, weights_s = trans_s(ts)\n",
    "            output_s_list.append(output_s)\n",
    "            s_weights_list.append(weights_s)\n",
    "\n",
    "        output_ts = torch.stack(output_ts_list)\n",
    "        output_ts = output_ts.permute(1, 2, 3, 0)\n",
    "        output_sensor = torch.stack(output_s_list)\n",
    "        output_sensor = output_sensor.permute(1, 0, 3, 2)\n",
    "\n",
    "        output_ts_sensor = output_ts + output_sensor\n",
    "\n",
    "        bs = output_ts_sensor.size()[0]\n",
    "\n",
    "        output_ts_sensor = output_ts_sensor.view(-1, output_ts_sensor.size()[2], output_ts_sensor.size()[3])\n",
    "        output_ts_sensor = output_ts_sensor.permute(0, 2, 1)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(output_ts_sensor))))\n",
    "\n",
    "        output_ts_sensor = output_ts_sensor + self.dropout2(src2)\n",
    "        src = self.norm2(output_ts_sensor)\n",
    "        src = src.permute(0, 2, 1)\n",
    "        src = src.reshape(bs, self.n_time_steps, src.size()[1] * src.size()[2])\n",
    "        src = src.mean(1)\n",
    "\n",
    "        x = self.dense1(src)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout_out1(x)\n",
    "        output = self.dense2(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def custom_MINMAX(batch_x, min_val, max_val):\n",
    "    '''Scales data between values (a,b)\n",
    "\n",
    "    Args:\n",
    "\n",
    "        batch_x (1D mumpy array): features data of the current batch\n",
    "        a (int): min value of transformed data\n",
    "        b (int): max value of transformed data\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        1D numpy array: Transformed data\n",
    "    '''\n",
    "    return (batch_x - min_val) / (max_val - min_val)\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torchmetrics import MetricCollection, Accuracy, Precision, Recall, F1,AveragePrecision\n",
    "\n",
    "# data_path = '/data/mcanizo/shap/point_anomaly_dataset.h5'\n",
    "data_path = '/data/mcanizo/data/simulations_dataset.h5'\n",
    "\n",
    "sequence_length = 8000       # the length of the time series\n",
    "window_length = 100          # the length (data points) of the window\n",
    "window_step = window_length  # indicates how much to slice de window over the time series\n",
    "time_steps = int(((sequence_length - window_length) / window_step) + 1)  # number of windows in which the time series are divided\n",
    "n_channels = 20              # number of sensors\n",
    "batch_size = 16             # the size of the batch\n",
    "n_epochs = 200                 # number of epochs in training\n",
    "n_conv_layers = 3            # number of convolutional layers\n",
    "conv_filters = 20            # number of filters on each convolutional layer\n",
    "n_rnn_layers = 2             # number of recurrent layers\n",
    "n_rnn_units = 128            # number of units on each recurrent layer\n",
    "rnn_layer_type = ['LSTM']   # the type of the recurrent layer (LSTM, Bi-LSTM, GRU, Bi-GRU, SimpleRNN)\n",
    "learning_rate = 0.001      # the value of the learning rate\n",
    "\n",
    "# set a random seed to reproduce the results over different executions\n",
    "random.seed(7)\n",
    "\n",
    "\n",
    "with h5py.File(data_path, 'r') as hf:\n",
    "    labels = hf['dataset'][:, -1]\n",
    "\n",
    "# get an ordered an ascending index of labels to be able to match each instance with its corresponding label\n",
    "index = np.array(range(len(labels)))\n",
    "data_index_list = np.array(range(len(labels)))\n",
    "normal_data_index = [index for index in data_index_list if labels[index] == 0]\n",
    "anomalous_data_index = [index for index in data_index_list if labels[index] == 1]\n",
    "\n",
    "np.random.shuffle(anomalous_data_index)\n",
    "n_anomalies_to_get = int(3 * len(labels) / 100)\n",
    "reduced_anomalous_data_index = anomalous_data_index[:n_anomalies_to_get]\n",
    "reduced_data_index = np.hstack((normal_data_index, reduced_anomalous_data_index))\n",
    "reduced_labels = labels[reduced_data_index]\n",
    "\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, train_size=0.7, random_state=42)\n",
    "for train_index, test_index in sss.split(reduced_data_index, reduced_labels):\n",
    "    # train_index and test_index are not the real indexes but new indexes made from reduced_data_index\n",
    "    real_train_index, real_test_index = reduced_data_index[train_index], reduced_data_index[test_index]\n",
    "    \n",
    "    \n",
    "    trainset = CustomDataGenerator(data_path, real_train_index, sequence_length, time_steps, window_length, window_step, n_channels)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=12)\n",
    "    testset = CustomDataGenerator(data_path, real_test_index, sequence_length, time_steps, window_length, window_step, n_channels)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=12)\n",
    "\n",
    "\n",
    "    # custom class to generate batch data on the fly\n",
    "    class CustomDataGenerator_minmax(Dataset):\n",
    "        def __init__(self, path, data_index, sequence_length, time_steps, window_length,\n",
    "                     window_step, n_channels):\n",
    "            \"\"\" Method called at the initialization of the class (Constructor).\n",
    "\n",
    "                Args:\n",
    "                    path (string): path where the dataset is located\n",
    "                    data_index (array): index list of the dataset\n",
    "                    batch_size (int): the size of the batch\n",
    "                    sequence_length (int): the length of the time series\n",
    "                    time_steps (int): number of windows in which the time series are divided\n",
    "                    window_length (int): the length (data point) of the window\n",
    "                    window_step (int): indicates how much to slice de window over the time series\n",
    "                    n_channels (int): number of sensors\n",
    "                    isTrainig (bool): whether the generator is for training or test\n",
    "\n",
    "            \"\"\"\n",
    "            self.path = path\n",
    "            self.data_index = data_index\n",
    "            self.sequence_length = sequence_length\n",
    "            self.time_steps = time_steps\n",
    "            self.window_length = window_length\n",
    "            self.window_step = window_step\n",
    "            self.n_channels = n_channels\n",
    "            self.len = data_index.shape[0]\n",
    "\n",
    "        def __len__(self):\n",
    "            \"\"\" Method called at the time of requiring the number of batches per epoch\n",
    "\n",
    "                    Returns:\n",
    "                        int: number of batches per epoch\n",
    "\n",
    "                \"\"\"\n",
    "            return self.len\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            index = self.data_index[idx]\n",
    "\n",
    "            col_size = self.sequence_length * self.n_channels\n",
    "            with h5py.File(self.path, 'r') as hf:\n",
    "                x = hf['dataset'][index, :col_size]\n",
    "                y = hf['dataset'][index, -1:]\n",
    "\n",
    "            # reshape data to 3D (time_steps* window_length, channels)\n",
    "            x = reshape_data_minmax(x, self.sequence_length, self.time_steps, self.window_length, self.window_step,\n",
    "                             self.n_channels)    \n",
    "            return x\n",
    "\n",
    "    def reshape_data_minmax(data, sequence_length, time_steps, window_length, window_step, n_channels):\n",
    "        x_reshaped = np.zeros((time_steps* window_length, n_channels))\n",
    "        for channel in range(n_channels):\n",
    "            channel_start_point = channel * sequence_length\n",
    "            channel_end_point = channel_start_point + sequence_length\n",
    "            param_values = data[channel_start_point:channel_end_point]\n",
    "            x_reshaped[:,channel] = param_values\n",
    "\n",
    "        return x_reshaped\n",
    "\n",
    "    def MinMax_total(trainloader,n_channels):\n",
    "        global_min = 1000000000000\n",
    "        global_max = -1000000000000\n",
    "        minmax_dict = {'channel_{}'.format(i): [global_min,global_max] for i in range(n_channels)}\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            batch_x = data\n",
    "#                 print(batch_x.shape)\n",
    "            for channel in range(n_channels):\n",
    "                channel_values = batch_x[:,:,channel]\n",
    "                min_batch = torch.amin(channel_values)\n",
    "                max_batch = torch.amax(channel_values)\n",
    "                if min_batch<minmax_dict['channel_{}'.format(channel)][0]:\n",
    "                    minmax_dict['channel_{}'.format(channel)][0] = min_batch\n",
    "                if  max_batch>minmax_dict['channel_{}'.format(channel)][1]:\n",
    "                    minmax_dict['channel_{}'.format(channel)][1] = max_batch\n",
    "        return minmax_dict\n",
    "\n",
    "    trainset_minmax = CustomDataGenerator_minmax(data_path, np.array(real_train_index), sequence_length, time_steps, window_length,\n",
    "                                   window_step, n_channels)\n",
    "    trainloader_minmax = torch.utils.data.DataLoader(trainset_minmax, batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=6)\n",
    "    minmax_dict =  MinMax_total(trainloader_minmax,n_channels)\n",
    "    \n",
    "    net = DFSTrans_model()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.00001)\n",
    "    cudnn.benchmark = True        \n",
    "    net = net.cuda()\n",
    "    f1_best = 0\n",
    "    recall_best = 0\n",
    "    precision_best = 0\n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        steps_per_epoch = 0\n",
    "        start_time = time.time()\n",
    "        n_samples = 0\n",
    "        \n",
    "        tp_all = 0\n",
    "        tn_all = 0\n",
    "        fp_all = 0\n",
    "        fn_all = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            inputs = torch.cat(inputs).view(13,inputs[0].size()[0],80,1,100)\n",
    "            for sensor in range(13):\n",
    "                channel_values = inputs[sensor].view(inputs[sensor].shape[0], -1)\n",
    "                min_val,max_val = minmax_dict['channel_{}'.format(sensor)]\n",
    "                scaled_data = custom_MINMAX(channel_values,min_val,max_val)\n",
    "                inputs[sensor] = torch.Tensor(scaled_data.float()).view(inputs[0].size()[0], 80, 1, 100)\n",
    "            \n",
    "            inputs, labels = Variable(inputs.cuda().type(torch.cuda.FloatTensor)), Variable(labels.cuda())\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            output = net(inputs)\n",
    "\n",
    "            loss = criterion(output.float(), labels.float())\n",
    "            output = (torch.sigmoid(output)>0.5).int()\n",
    "            tp = (labels * output).sum().to(torch.float32)\n",
    "            tn = ((1 - labels) * (1 - output)).sum().to(torch.float32)\n",
    "            fp = ((1 - labels) * output).sum().to(torch.float32)\n",
    "            fn = (labels * (1 - output)).sum().to(torch.float32)\n",
    "            tp_all+=tp\n",
    "            tn_all+=tn\n",
    "            fp_all+=fp\n",
    "            fn_all+=fn\n",
    "    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            steps_per_epoch += 1\n",
    "            \n",
    "        precision = tp_all / (tp_all + fp_all)\n",
    "        recall = tp_all / (tp_all + fn_all)\n",
    "\n",
    "        f1 = 2* (precision*recall) / (precision + recall)\n",
    "        \n",
    "        print('Epoch %d loss: %.5f ' % (epoch+1,running_loss/steps_per_epoch))\n",
    "        print('Precision train %.5f , Recall train %.5f , F1 train %.5f' % (precision,recall,f1))\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tp_all = 0\n",
    "            tn_all = 0\n",
    "            fp_all = 0\n",
    "            fn_all = 0            \n",
    "            val_loss = 0.0\n",
    "            steps_per_epoch = 0\n",
    "            start_time = time.time()\n",
    "            correct = 0\n",
    "            for i, data in enumerate(testloader, 0):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs, labels = data\n",
    "                inputs = torch.cat(inputs).view(13,inputs[0].size()[0],80,1,100)\n",
    "                for sensor in range(13):\n",
    "                    channel_values = inputs[sensor].view(inputs[sensor].shape[0], -1)\n",
    "                    min_val,max_val = minmax_dict['channel_{}'.format(sensor)]\n",
    "                    scaled_data = custom_MINMAX(channel_values,min_val,max_val)\n",
    "                    inputs[sensor] = torch.Tensor(scaled_data.float()).view(inputs[0].size()[0], 80, 1, 100)\n",
    "\n",
    "                inputs, labels = Variable(inputs.cuda().type(torch.cuda.FloatTensor)), Variable(labels.cuda())\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                output = net(inputs)\n",
    "\n",
    "                output = output.flatten()\n",
    "                labels = labels.flatten()\n",
    "                output = output.view(output.size()[0],1)\n",
    "                labels = labels.view(labels.size()[0],1)\n",
    "\n",
    "                loss = criterion(output.float(), labels.float())\n",
    "                output = (torch.sigmoid(output)>0.5).int()\n",
    "\n",
    "                tp = (labels * output).sum().to(torch.float32)\n",
    "                tn = ((1 - labels) * (1 - output)).sum().to(torch.float32)\n",
    "                fp = ((1 - labels) * output).sum().to(torch.float32)\n",
    "                fn = (labels * (1 - output)).sum().to(torch.float32)\n",
    "\n",
    "                tp_all+=tp\n",
    "                tn_all+=tn\n",
    "                fp_all+=fp\n",
    "                fn_all+=fn\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                steps_per_epoch += 1\n",
    "            \n",
    "            \n",
    "            precision = tp_all / (tp_all + fp_all)\n",
    "            recall = tp_all / (tp_all + fn_all)\n",
    "\n",
    "            f1 = 2* (precision*recall) / (precision + recall)\n",
    "\n",
    "            print('test loss: %.5f ' % (val_loss/steps_per_epoch))\n",
    "            print('Precision test %.5f , Recall test %.5f , F1 test %.5f' % (precision,recall,f1))\n",
    "            \n",
    "            if f1 > f1_best:\n",
    "                f1_best = f1\n",
    "                checkpoint = {'epoch': epoch + 1,\n",
    "                          'state_dict': net.state_dict(),\n",
    "                          'optimizer': optimizer.state_dict()}\n",
    "                torch.save(checkpoint, 'DFStrans_reduced_sensors.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761ea76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02668c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DFSTrans_model(nn.Module):\n",
    "\n",
    "    def __init__(self, activation=\"relu\", d_model=240, dim_feedforward=2048,\n",
    "                 dropout=0.1,n_channels = 13,n_time_steps = 80,output_dim = 3120, n_units_l1 = 512, conv_filters = 20):\n",
    "        super(DFSTrans_model, self).__init__()\n",
    "        self.conv_cell = nn.ModuleList([MultiHead1DCNN(conv_filters = conv_filters, time_steps = n_time_steps) for i in range(n_channels)])\n",
    "        self.TimeDistributed_flatten = nn.ModuleList([TimeDistributed(Flatten) for i in range(n_channels)])\n",
    "        self.trace = []\n",
    "        self.TransformerTS_list = nn.ModuleList([TransTS() for i in range(n_channels)])\n",
    "        self.TransformerS_list = nn.ModuleList([TransSensor() for i in range(n_time_steps)])\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.pos_encoder = DFTEncoding(self.d_model)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.dense1 = nn.Linear(output_dim, n_units_l1)\n",
    "        self.dropout_out1 = nn.Dropout(dropout)\n",
    "        self.dense2 = nn.Linear(n_units_l1, 1)\n",
    "\n",
    "        self.n_time_steps = n_time_steps\n",
    "        self.n_channels = n_channels\n",
    "\n",
    "    def forward(self, input_x):\n",
    "\n",
    "        trace = []\n",
    "        for sensor_n in range(self.n_channels):\n",
    "            input_layer = input_x[sensor_n]\n",
    "            input_layer_reshape = input_layer.view(-1, *(input_layer.size()[2:]))\n",
    "            x = self.conv_cell[sensor_n](input_layer_reshape)\n",
    "            x = x.view(x.size()[0] // self.n_time_steps, self.n_time_steps, -1)\n",
    "            trace.append(x)\n",
    "\n",
    "        x = torch.stack(trace)\n",
    "        x = x.permute(2, 1, 3, 0)\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        input_ts = torch.clone(x)\n",
    "        input_sensor = torch.clone(x)\n",
    "        input_ts = input_ts.permute(3, 0, 1, 2)\n",
    "        input_sensor = input_sensor.permute(0, 3, 1, 2)\n",
    "\n",
    "        output_ts_list = []\n",
    "        ts_weights_list = []\n",
    "\n",
    "        for i in range(self.n_channels):\n",
    "            channel = input_ts[i, :, :, :]\n",
    "            trans_ts = self.TransformerTS_list[i]\n",
    "\n",
    "            output_ts, weights_ts = trans_ts(channel)\n",
    "            output_ts_list.append(output_ts)\n",
    "            ts_weights_list.append(weights_ts)\n",
    "\n",
    "        output_s_list = []\n",
    "        s_weights_list = []\n",
    "\n",
    "        for i in range(self.n_time_steps):\n",
    "            ts = input_sensor[i, :, :, :]\n",
    "            trans_s = self.TransformerS_list[i]\n",
    "\n",
    "            output_s, weights_s = trans_s(ts)\n",
    "            output_s_list.append(output_s)\n",
    "            s_weights_list.append(weights_s)\n",
    "\n",
    "        output_ts = torch.stack(output_ts_list)\n",
    "        output_ts = output_ts.permute(1, 2, 3, 0)\n",
    "        output_sensor = torch.stack(output_s_list)\n",
    "        output_sensor = output_sensor.permute(1, 0, 3, 2)\n",
    "\n",
    "        output_ts_sensor = output_ts + output_sensor\n",
    "\n",
    "        bs = output_ts_sensor.size()[0]\n",
    "\n",
    "        output_ts_sensor = output_ts_sensor.view(-1, output_ts_sensor.size()[2], output_ts_sensor.size()[3])\n",
    "        output_ts_sensor = output_ts_sensor.permute(0, 2, 1)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(output_ts_sensor))))\n",
    "\n",
    "        output_ts_sensor = output_ts_sensor + self.dropout2(src2)\n",
    "        src = self.norm2(output_ts_sensor)\n",
    "        src = src.permute(0, 2, 1)\n",
    "        src = src.reshape(bs, self.n_time_steps, src.size()[1] * src.size()[2])\n",
    "        src = src.mean(1)\n",
    "\n",
    "        x = self.dense1(src)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout_out1(x)\n",
    "        output = self.dense2(x)\n",
    "\n",
    "        return output,ts_weights_list,s_weights_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5b80f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DFSTrans_model(\n",
       "  (conv_cell): ModuleList(\n",
       "    (0): MultiHead1DCNN(\n",
       "      (conv1d1): Conv1d(1, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv1d2): Conv1d(20, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv1d3): Conv1d(20, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn3): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): MultiHead1DCNN(\n",
       "      (conv1d1): Conv1d(1, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv1d2): Conv1d(20, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv1d3): Conv1d(20, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn3): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (2): MultiHead1DCNN(\n",
       "      (conv1d1): Conv1d(1, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv1d2): Conv1d(20, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv1d3): Conv1d(20, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn3): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (3): MultiHead1DCNN(\n",
       "      (conv1d1): Conv1d(1, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv1d2): Conv1d(20, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv1d3): Conv1d(20, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn3): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (4): MultiHead1DCNN(\n",
       "      (conv1d1): Conv1d(1, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv1d2): Conv1d(20, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv1d3): Conv1d(20, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn3): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (5): MultiHead1DCNN(\n",
       "      (conv1d1): Conv1d(1, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv1d2): Conv1d(20, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv1d3): Conv1d(20, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn3): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (6): MultiHead1DCNN(\n",
       "      (conv1d1): Conv1d(1, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv1d2): Conv1d(20, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv1d3): Conv1d(20, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn3): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (7): MultiHead1DCNN(\n",
       "      (conv1d1): Conv1d(1, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv1d2): Conv1d(20, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv1d3): Conv1d(20, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn3): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (8): MultiHead1DCNN(\n",
       "      (conv1d1): Conv1d(1, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv1d2): Conv1d(20, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv1d3): Conv1d(20, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn3): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (9): MultiHead1DCNN(\n",
       "      (conv1d1): Conv1d(1, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv1d2): Conv1d(20, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv1d3): Conv1d(20, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn3): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (10): MultiHead1DCNN(\n",
       "      (conv1d1): Conv1d(1, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv1d2): Conv1d(20, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv1d3): Conv1d(20, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn3): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (11): MultiHead1DCNN(\n",
       "      (conv1d1): Conv1d(1, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv1d2): Conv1d(20, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv1d3): Conv1d(20, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn3): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (12): MultiHead1DCNN(\n",
       "      (conv1d1): Conv1d(1, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv1d2): Conv1d(20, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (conv1d3): Conv1d(20, 20, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn3): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (TimeDistributed_flatten): ModuleList(\n",
       "    (0): TimeDistributed()\n",
       "    (1): TimeDistributed()\n",
       "    (2): TimeDistributed()\n",
       "    (3): TimeDistributed()\n",
       "    (4): TimeDistributed()\n",
       "    (5): TimeDistributed()\n",
       "    (6): TimeDistributed()\n",
       "    (7): TimeDistributed()\n",
       "    (8): TimeDistributed()\n",
       "    (9): TimeDistributed()\n",
       "    (10): TimeDistributed()\n",
       "    (11): TimeDistributed()\n",
       "    (12): TimeDistributed()\n",
       "  )\n",
       "  (TransformerTS_list): ModuleList(\n",
       "    (0): TransTS(\n",
       "      (encoder_layer): TemporalEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): TemporalEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransTS(\n",
       "      (encoder_layer): TemporalEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): TemporalEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransTS(\n",
       "      (encoder_layer): TemporalEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): TemporalEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransTS(\n",
       "      (encoder_layer): TemporalEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): TemporalEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransTS(\n",
       "      (encoder_layer): TemporalEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): TemporalEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransTS(\n",
       "      (encoder_layer): TemporalEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): TemporalEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransTS(\n",
       "      (encoder_layer): TemporalEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): TemporalEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransTS(\n",
       "      (encoder_layer): TemporalEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): TemporalEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TransTS(\n",
       "      (encoder_layer): TemporalEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): TemporalEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TransTS(\n",
       "      (encoder_layer): TemporalEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): TemporalEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TransTS(\n",
       "      (encoder_layer): TemporalEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): TemporalEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): TransTS(\n",
       "      (encoder_layer): TemporalEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): TemporalEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (12): TransTS(\n",
       "      (encoder_layer): TemporalEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): TemporalEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (TransformerS_list): ModuleList(\n",
       "    (0): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (12): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (13): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (14): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (15): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (16): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (17): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (18): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (19): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (20): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (21): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (22): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (23): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (24): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (25): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (26): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (27): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (28): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (29): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (30): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (31): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (32): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (33): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (34): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (35): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (36): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (37): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (38): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (39): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (40): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (41): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (42): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (43): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (44): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (45): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (46): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (47): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (48): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (49): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (50): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (51): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (52): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (53): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (54): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (55): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (56): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (57): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (58): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (59): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (60): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (61): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (62): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (63): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (64): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (65): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (66): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (67): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (68): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (69): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (70): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (71): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (72): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (73): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (74): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (75): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (76): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (77): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (78): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (79): TransSensor(\n",
       "      (encoder_layer): SpatialEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SpatialEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pos_encoder): DFTEncoding()\n",
       "  (sigmoid): Sigmoid()\n",
       "  (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (linear2): Linear(in_features=2048, out_features=240, bias=True)\n",
       "  (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  (dense1): Linear(in_features=3120, out_features=512, bias=True)\n",
       "  (dropout_out1): Dropout(p=0.1, inplace=False)\n",
       "  (dense2): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torchmetrics import MetricCollection, Accuracy, Precision, Recall, F1,AveragePrecision\n",
    "# data_path = '/data/mcanizo/shap/point_anomaly_dataset.h5'\n",
    "\n",
    "\n",
    "data_path = '/data/mcanizo/data/simulations_dataset.h5'\n",
    "\n",
    "sequence_length = 8000       # the length of the time series\n",
    "window_length = 100          # the length (data points) of the window\n",
    "window_step = window_length  # indicates how much to slice de window over the time series\n",
    "time_steps = int(((sequence_length - window_length) / window_step) + 1)  # number of windows in which the time series are divided\n",
    "n_channels = 20              # number of sensors\n",
    "batch_size = 16             # the size of the batch\n",
    "n_epochs = 3                 # number of epochs in training\n",
    "n_conv_layers = 3            # number of convolutional layers\n",
    "conv_filters = 20            # number of filters on each convolutional layer\n",
    "n_rnn_layers = 2             # number of recurrent layers\n",
    "n_rnn_units = 128            # number of units on each recurrent layer\n",
    "rnn_layer_type = ['LSTM']   # the type of the recurrent layer (LSTM, Bi-LSTM, GRU, Bi-GRU, SimpleRNN)\n",
    "learning_rate = 0.001      # the value of the learning rate\n",
    "# dom seed to reproduce the results over different executions\n",
    "random.seed(7)\n",
    "\n",
    "\n",
    "with h5py.File(data_path, 'r') as hf:\n",
    "    labels = hf['dataset'][:, -1]\n",
    "\n",
    "# get an ordered an ascending index of labels to be able to match each instance with its corresponding label\n",
    "index = np.array(range(len(labels)))\n",
    "\n",
    "data_index_list = np.array(range(len(labels)))\n",
    "normal_data_index = [index for index in data_index_list if labels[index] == 0]\n",
    "anomalous_data_index = [index for index in data_index_list if labels[index] == 1]\n",
    "\n",
    "\n",
    "np.random.shuffle(anomalous_data_index)\n",
    "n_anomalies_to_get = int(60 * len(labels) / 100)\n",
    "reduced_anomalous_data_index = anomalous_data_index[:n_anomalies_to_get]\n",
    "\n",
    "reduced_data_index = np.hstack((normal_data_index, reduced_anomalous_data_index))\n",
    "reduced_labels = labels[reduced_data_index]\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, train_size=0.7, random_state=42)\n",
    "for train_index, test_index in sss.split(reduced_data_index, reduced_labels):\n",
    "    \n",
    "    trainset = CustomDataGenerator(data_path, train_index, sequence_length, time_steps, window_length, window_step, n_channels)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=100,\n",
    "                                          shuffle=True, num_workers=12)\n",
    "    testset = CustomDataGenerator(data_path, test_index, sequence_length, time_steps, window_length, window_step, n_channels)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
    "                                          shuffle=False, num_workers=12)\n",
    "    \n",
    "\n",
    "\n",
    "net = DFSTrans_model()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.00001)\n",
    "net = net.cuda()\n",
    "checkpoint = torch.load('DFStrans_reduced_sensors.pt')\n",
    "\n",
    "net.load_state_dict(checkpoint['state_dict'],strict=False)\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "epoch = checkpoint['epoch']\n",
    "net.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d2a48c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False]], device='cuda:0')\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_batch = 0\n",
    "with torch.no_grad():\n",
    "\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "\n",
    "        inputs, labels = data\n",
    "\n",
    "        if 1 in labels:\n",
    "            if n_batch == 0: \n",
    "\n",
    "                inputs = torch.cat(inputs).view(13,inputs[0].size()[0],80,1,100)\n",
    "                for sensor in range(13):\n",
    "                    channel_values = inputs[sensor].view(inputs[sensor].shape[0], -1)\n",
    "                    min_val,max_val = minmax_dict['channel_{}'.format(sensor)]\n",
    "                    scaled_data = custom_MINMAX(channel_values,min_val,max_val)\n",
    "                    inputs[sensor] = torch.Tensor(scaled_data.float()).view(inputs[0].size()[0], 80, 1, 100)\n",
    "                inputs, labels = Variable(inputs.cuda().type(torch.cuda.FloatTensor)), Variable(labels.cuda().type(torch.cuda.FloatTensor))\n",
    "                # zero the parameter gradients\n",
    "                # forward + backward + optimize\n",
    "\n",
    "                output, weights_ts, weights_sensor = net(inputs)\n",
    "                weights_ts = torch.stack(weights_ts)\n",
    "                weights_sensor = torch.stack(weights_sensor)\n",
    "                \n",
    "                break\n",
    "            n_batch+=1\n",
    "\n",
    "weights_ts = weights_ts.permute(1,0,2,3)\n",
    "weights_sensor = weights_sensor.permute(1,0,2,3)\n",
    "\n",
    "print(torch.sigmoid(output)>0.5)\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce5e1a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b02a45",
   "metadata": {},
   "source": [
    "**GLOBAL INTERPRETATIONS: WHEN AND WHERE THE ANOMALY HAS OCURRED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bcc19c9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOMALOUS INDEXES:  [11 16 20 24 36 40 49 67 71 75 79 81 88 90 91 96 99]\n",
      "NON-ANOMALOUS INDEXES:  [ 0  1  2  3  4  5  6  7  8  9 10 12 13 14 15 17 18 19 21 22 23 25 26 27\n",
      " 28 29 30 31 32 33 34 35 37 38 39 41 42 43 44 45 46 47 48 50 51 52 53 54\n",
      " 55 56 57 58 59 60 61 62 63 64 65 66 68 69 70 72 73 74 76 77 78 80 82 83\n",
      " 84 85 86 87 89 92 93 94 95 97 98]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3336dbe7f246bb95e2a31bbca0c27f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='idx', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly import tools,subplots\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "import h5py\n",
    "from random import uniform\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "%matplotlib inline\n",
    "# weights_ts = weights_ts.permute(1,0,2,3)\n",
    "\n",
    "# print(np.where(labels.cpu()==1))\n",
    "def plot_global_att(idx,type_of_explanation,average):\n",
    "    font = {'family': 'serif',\n",
    "        'color':  'black',\n",
    "        'weight': 'normal',\n",
    "        'size': 30,\n",
    "        }\n",
    "    n_channels = 20\n",
    "    sequence_length = 8000\n",
    "    output_params = {'Ax':0, 'Ay':1, 'Az':2, 'Id':3, 'Iq':4,'Om':5, 'Phi':6, 'Vc':7, 'Vcw':8, 'Vd':9, 'Vq':10, 'Zc':11, 'Zcw':12}\n",
    "    params_to_show = ['Ax', 'Ay', 'Az', 'Id', 'Iq','Om', 'Phi', 'Vc', 'Vcw', 'Vd', 'Vq', 'Zc', 'Zcw']\n",
    "\n",
    "    x = list(range(sequence_length))\n",
    "    simulation_id = 0\n",
    "\n",
    "    data = inputs.permute(1,0,2,3,4)\n",
    "    fig = tools.make_subplots(rows=len(params_to_show), cols=1, shared_xaxes=True)\n",
    "    plot_row = 0\n",
    "    \n",
    "    for param in params_to_show:\n",
    "        channel_number = output_params[param]\n",
    "        data_channel = data[idx,channel_number].flatten().cpu()\n",
    "        trace = go.Scatter(x=x, y=data_channel, name=param)\n",
    "        plot_row += 1\n",
    "        fig.append_trace(trace, plot_row, 1)\n",
    "        \n",
    "    for row in range(len(params_to_show)):\n",
    "        fig.update_yaxes(title_text=params_to_show[row], row=row+1, col=1,titlefont=dict(size=40))\n",
    "\n",
    "\n",
    "    fig.update_layout(legend=dict(title=\"\",font=dict(size = 10),\n",
    "        orientation=\"h\"),legend_title=dict(font=dict(size=12)),)\n",
    "    fig.update_layout(\n",
    "        autosize=False,\n",
    "        width=1000,\n",
    "        height=1000,)\n",
    "    fig.update_xaxes(title=\"\",tickfont=dict(size=20), titlefont=dict(size=20),linecolor=\"black\", showline=True, linewidth=2,  gridcolor='#BCCCDC')\n",
    "\n",
    "    fig.update_layout(showlegend=False, plot_bgcolor='rgba(0,0,0,0)',legend=dict(title=\"\",font=dict(size = 30)),)\n",
    "    fig.show()\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.style.use('default')\n",
    "\n",
    "\n",
    "    if type_of_explanation == 'Temporal':\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.rcParams['axes.facecolor']='w'\n",
    "#         plt.grid(True,color = 'grey')\n",
    "        plt.imshow(weights_ts[idx].mean(0).detach().cpu().numpy())\n",
    "        plt.colorbar(fraction=0.046, pad=0.04)\n",
    "\n",
    "        plt.xlabel('Global temporal attention matrix, $A_{t \\prime ,t}^{\\mathrm{G}}$',fontdict=font)\n",
    "        plt.savefig('global_temp_matrix_FJ_{}.pdf'.format(idx),bbox_inches='tight',fontdict=font)  \n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10,10))    \n",
    "        plt.rcParams['axes.facecolor']='w'\n",
    "        plt.grid(True,color = 'grey')\n",
    "        plt.bar(np.arange(80),np.sum(weights_ts[idx].mean(0).detach().cpu().numpy(),axis=0))\n",
    "        plt.xlabel('Global temporal relevance scores, $a^{\\mathrm{G}}_t$',fontdict=font)\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    if type_of_explanation == 'Spatial':\n",
    "        \n",
    "            weights_by_ts = np.sum(weights_ts[idx].mean(0).detach().cpu().numpy(),axis=0)\n",
    "\n",
    "            weighted_avg_sensor = np.average(weights_sensor[idx].detach().cpu().numpy(), weights=weights_by_ts,axis=0)\n",
    "            plt.figure(figsize=(10,10))\n",
    "            plt.rcParams['axes.facecolor']='w'\n",
    "            plt.imshow(weighted_avg_sensor)\n",
    "            plt.colorbar(fraction=0.046, pad=0.04)\n",
    "            plt.xlabel('Global spatial attention matrix, $B_{s \\prime, s}^{\\mathrm{G}} $ ',fontdict=font)\n",
    "            labels = params_to_show\n",
    "            plt.xticks([i for i in range(len(labels))],labels,fontsize=30, rotation=90)\n",
    "            plt.yticks([i for i in range(len(labels))],labels,fontsize=30)\n",
    "            plt.show()\n",
    "\n",
    "            labels_n = labels\n",
    "#             labels_n = ['Ax', 'Id', 'Iq','Vq','Vd','Phi', 'Az','Zc', 'Vc', 'Zcw','Vcw','Om','Ay']\n",
    "#             labels_n = [labels_n[len(labels_n)-i-1] for i in range(len(labels_n)) ]\n",
    "            plt.figure()\n",
    "            plt.rcParams['axes.facecolor']='w'\n",
    "            plt.grid(True,color = 'grey')\n",
    "            shap.summary_plot(weighted_avg_sensor,\n",
    "                              weighted_avg_sensor,\n",
    "                              params_to_show, plot_type='bar',show=False)\n",
    "            plt.xlabel('Global spatial relevance scores, $ b^{\\mathrm{G}}_s$',fontdict=font)\n",
    "            plt.yticks([i for i in range(len(labels_n))],labels_n,fontsize=30)\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "bs = 100\n",
    "\n",
    "normal_idx = np.where(labels.cpu()==0)[0]\n",
    "anom_idx = np.where(labels.cpu()==1)[0]\n",
    "\n",
    "print('ANOMALOUS INDEXES: ',anom_idx)\n",
    "print('NON-ANOMALOUS INDEXES: ',normal_idx)\n",
    "        \n",
    "interactive(plot_global_att, idx=[i for i in range(bs)],type_of_explanation = ['Temporal','Spatial'],average = ['Weighted'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f7d6c0",
   "metadata": {},
   "source": [
    "**LOCAL INTERPRETATIONS: At what time-step is focusing for a particular sensor?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bf83447",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOMALOUS INDEXES:  [11 16 20 24 36 40 49 67 71 75 79 81 88 90 91 96 99]\n",
      "NON-ANOMALOUS INDEXES:  [ 0  1  2  3  4  5  6  7  8  9 10 12 13 14 15 17 18 19 21 22 23 25 26 27\n",
      " 28 29 30 31 32 33 34 35 37 38 39 41 42 43 44 45 46 47 48 50 51 52 53 54\n",
      " 55 56 57 58 59 60 61 62 63 64 65 66 68 69 70 72 73 74 76 77 78 80 82 83\n",
      " 84 85 86 87 89 92 93 94 95 97 98]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbdad2866be74fa9808e88136983393a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='sensor_name', options=('Iq',), value='Iq'), Dropdown(description='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly import tools,subplots\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "import h5py\n",
    "from random import uniform\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "# import plotly.express as px\n",
    "\n",
    "normal_idx = np.where(labels.cpu()==0)[0]\n",
    "anom_idx = np.where(labels.cpu()==1)[0]\n",
    "\n",
    "print('ANOMALOUS INDEXES: ',anom_idx)\n",
    "print('NON-ANOMALOUS INDEXES: ',normal_idx)\n",
    "def local_temporal_att(sensor_name,anom_index):\n",
    "    n_channels = 20\n",
    "    sequence_length = 8000\n",
    "    output_params = {'Ax':0, 'Ay':1, 'Az':2, 'Id':3, 'Iq':4,'Omega':5, 'Phi':6, 'Vc':7, 'Vcw':8, 'Vd':9, 'Vq':10, 'Zc':11, 'Zcw':12}\n",
    "    params_to_show = ['Ax', 'Ay', 'Az', 'Id', 'Iq','Omega', 'Phi', 'Vc', 'Vcw', 'Vd', 'Vq', 'Zc', 'Zcw']\n",
    "\n",
    "    x = list(range(sequence_length))\n",
    "    simulation_id = 0\n",
    "\n",
    "    data = inputs.permute(1,0,2,3,4)\n",
    "\n",
    "    x = list(range(sequence_length))\n",
    "    colors = ['blue','red','orange','green','purple']\n",
    "\n",
    "\n",
    "    fig = subplots.make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "    plot_row = 0\n",
    "    data = inputs.permute(1,0,2,3,4)\n",
    "    sensors_dict = {'Ax':0, 'Ay':1, 'Az':2, 'Id':3, 'Iq':4,'Omega':5, 'Phi':6, 'Vc':7, 'Vcw':8, 'Vd':9, 'Vq':10, 'Zc':11, 'Zcw':12}\n",
    "    sensor_id = sensors_dict[sensor_name]\n",
    "    n = 0\n",
    "    for anom_index in [11,16,20,24,36]:\n",
    "        bars = np.sum(weights_ts[anom_index][sensor_id].detach().cpu().numpy(),axis=0)\n",
    "        zero_idx = np.argsort(bars)[:-60]\n",
    "        \n",
    "#         bars[zero_idx] = 0\n",
    "        \n",
    "        \n",
    "        bars_extended = np.repeat(bars,100)\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=np.arange(8000), y=bars_extended, mode='lines', name=sensor_name,\n",
    "                                 line={'color':colors[n],'width':2}),secondary_y=True)\n",
    "\n",
    "        \n",
    "        if sensor_selected=='Iq':\n",
    "            if anom_index not in [4,5]:\n",
    "                data_channel = data[anom_index,sensor_id].flatten().cpu()+0.5-0.2*n\n",
    "            else:\n",
    "                data_channel = data[anom_index,sensor_id].flatten().cpu()+0.5-0.05*n\n",
    "        else:\n",
    "             data_channel = data[anom_index,sensor_id].flatten().cpu()+0.5-0.3*n\n",
    "\n",
    "        trace = go.Scatter(x=x, y=data_channel, name=sensor_name,line=dict(color=colors[n]))\n",
    "        fig.add_trace(trace,secondary_y=False)\n",
    "\n",
    "        n+=1\n",
    "        \n",
    "\n",
    "\n",
    "        fig.update_traces(marker_line_width=1.5, opacity=0.6)\n",
    "\n",
    "        fig.update_layout(legend=dict(title=\"\",font=dict(size = 20),\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"top\",\n",
    "            y=1,\n",
    "            xanchor=\"right\",\n",
    "            x=0.97\n",
    "        ),legend_title=dict(font=dict(size=12)),)\n",
    "        fig.update_layout(\n",
    "            autosize=False,\n",
    "            width=1000,\n",
    "            height=500,)\n",
    "        fig.update_xaxes(title=\"\",tickfont=dict(size=20), titlefont=dict(size=20),linecolor=\"black\", showline=True, linewidth=2,  gridcolor='#BCCCDC')\n",
    "        fig.update_layout(showlegend=False, plot_bgcolor='rgba(0,0,0,0)',legend=dict(title=\"\",font=dict(size = 20)),yaxis_range=[-1.5,1.5])\n",
    "        fig.update_yaxes(visible=False)\n",
    "\n",
    "\n",
    "        print('THESE ARE THE MOST INFLUENTIAL TIME STEPS FOR SENSOR {}: '.format(sensor_name))\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.rcParams['axes.facecolor']='w'\n",
    "        plt.imshow(weights_ts[anom_index][sensor_id].detach().cpu().numpy())\n",
    "        plt.colorbar(fraction=0.046, pad=0.04)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.rcParams['axes.facecolor']='w'\n",
    "        plt.grid(True,color = 'grey')\n",
    "        plt.bar(np.arange(80),np.sum(weights_ts[anom_index][sensor_id].detach().cpu().numpy(),axis=0))\n",
    "    fig.show()\n",
    "\n",
    "sensor_selected = 'Iq'\n",
    "interactive(local_temporal_att, sensor_name= [sensor_selected], anom_index=[i for i in range(100)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57218321",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOMALOUS INDEXES:  [11 16 20 24 36 40 49 67 71 75 79 81 88 90 91 96 99]\n",
      "NON-ANOMALOUS INDEXES:  [ 0  1  2  3  4  5  6  7  8  9 10 12 13 14 15 17 18 19 21 22 23 25 26 27\n",
      " 28 29 30 31 32 33 34 35 37 38 39 41 42 43 44 45 46 47 48 50 51 52 53 54\n",
      " 55 56 57 58 59 60 61 62 63 64 65 66 68 69 70 72 73 74 76 77 78 80 82 83\n",
      " 84 85 86 87 89 92 93 94 95 97 98]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f7b74c80095469db5e2cfbb6ae2575d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='time_step', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly import tools,subplots\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "import h5py\n",
    "from random import uniform\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "normal_idx = np.where(labels.cpu()==0)[0]\n",
    "anom_idx = np.where(labels.cpu()==1)[0]\n",
    "\n",
    "print('ANOMALOUS INDEXES: ',anom_idx)\n",
    "print('NON-ANOMALOUS INDEXES: ',normal_idx)\n",
    "\n",
    "def local_spatial_att(time_step,anom_index):\n",
    "\n",
    "    a = np.sum(weights_ts[anom_index].mean(0).detach().cpu().numpy(),axis=0)\n",
    "    print('MOST INFLUENTIAL TIME STEP: ',np.where(a==max(a)))\n",
    "    \n",
    "    print('THESE ARE THE MOST INFLUENTIAL SENSORS AT TIME-STEP {}: '.format(time_step))\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.imshow(weights_sensor[anom_index][time_step].detach().cpu().numpy())\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.clim(0,0.15)\n",
    "        \n",
    "    params_to_show = ['Ax', 'Ay', 'Az', 'Id', 'Iq','Omega', 'Phi', 'Vc', 'Vcw', 'Vd', 'Vq', 'Zc', 'Zcw']\n",
    "\n",
    "    plt.figure()\n",
    "    shap.summary_plot(weights_sensor[anom_index][time_step].detach().cpu().numpy(),\n",
    "                      weights_sensor[anom_index][time_step].detach().cpu().numpy(),\n",
    "                      params_to_show, plot_type='bar',show=False)\n",
    "    plt.xlabel('mean(attention weights per sensor) (average impact on the model output magnitude)')\n",
    "\n",
    "interactive(local_spatial_att, time_step=[i for i in range(80)], anom_index=[i for i in range(100)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
